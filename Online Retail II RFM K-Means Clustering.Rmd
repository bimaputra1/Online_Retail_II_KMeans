---
title: "Online Retail II RFM K-Means Clustering"
author: "Bima Putra Pratama"
date: "10/6/2019"
output:
  pdf_document:
    toc: true
    toc_depth: 3
---

# Overview 

This project is a part of final project from the HarvardX PH125.9x Data Science:Capstone course by Rafael Irizarry. The aim of this project is to do customer segmentation analysis from online retail II dataset from UCI ML repository. This analysis will focus on getting RFM values and clustering it using K-Means algorithms.

Customer segmentation is a method to grouping customers based on desired criteria. In this project the customers was divided into some groups based on their recency, frequency and monetary value. Recency is about when the last time customers make an order. It means the number of days since a customer made the last purcase. Frequency is the total number of customer purcase in a given period. Then monetary is the total amount of money customer spent in that period. These three values that are used as features to conduct K-Means clustering.

Several step was taken to complete this project. First is prepare the library and download the required dataset. Then the raw data are cleaned up and scaled use standardization and normalization prior to modelling. Furthermore the optimum values of cluster was determined by produced an elbow plot. Lastly the modelling was performed and summarized the results.

# Dataset

## Dataset Preparation

This project use Online Retail II dataset that will be downloaded from UCI Repository here [http://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx](http://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx). This dataset contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011.The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers. However this project only used the data from year 2010 - 2011 in the second worksheet of this dataset. 

\pagebreak
```{r import_dataset, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

# Prepare Required 
if(!require(tidyverse)) install.packages("tidyverse", 
                                         repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", 
                                     repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", 
                                          repos = "http://cran.us.r-project.org")
if(!require(GGally)) install.packages("GGally", 
                                          repos = "http://cran.us.r-project.org")

# Import Dataset
files <- tempfile()
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx", files)
df <- read_excel(files, sheet = 'Year 2010-2011', col_names = TRUE)
df <- df %>% rename(CustomerID = `Customer ID`) # Rename CustomerID Column Names
glimpse(df)

```

Data summaries can be seen to get initial understanding as follow:
```{r summary, echo=TRUE, eval = TRUE}
summary(df)
```

As can be seen on the summary of the dataset, several step need to be taken to prepare the dataset before clustering the data. Firstly, row's with negative values in the 'Quantity' and 'Price' columns will be removed. Then row's with NA's values also will be excluded from this dataset. Furthermore some column also need to be recoded to factor and changes the date and time to date type only prior to clustering by code below:
```{r clean_up, echo=TRUE, eval = TRUE}
# Remove Rows that have Negative Values of Quantity and Price
clean_df <- df %>%
  filter(Quantity > 0 & Price > 0) %>% 
  drop_na()

# Recode Dataset
Recode_df <- clean_df %>% 
  mutate(Invoice = as.factor(Invoice), StockCode = as.factor(StockCode),
         InvoiceDate = date(InvoiceDate), CustomerID = as.factor(CustomerID), 
         Country = as.factor(Country))

summary(Recode_df)
```

Lastly, the total spend will be calculated by multiply price and quantity per transaction.

```{r total_spend, echo=TRUE, eval = TRUE}
Recode_df <- Recode_df %>% 
  mutate(TotalSpend = Quantity * Price)

summary(Recode_df)
```

## RFM Data Preparation

In order to perform the RFM Analysis, further process to the data set is required as follows:
1. Get the day after last InvoiceDate and used it as reference date.
2. Find the latest transaction date and calculate the day to the reference date per customer to get recency values.
3. Determine how many transaction has been done per customer and named as frequency values.
4. Total spend per customer that named as monetary values.

```{r RFM_data, echo=TRUE, eval = TRUE}

# Get Analysis Reference Date (One Day After Last Transaction)
Max_Date <- date(max(Recode_df$InvoiceDate)) + 1

# Calculate RFM Values 
rfm_df <- Recode_df %>% 
  group_by(CustomerID) %>% 
  summarise(recency = as.numeric(Max_Date - max(InvoiceDate)),
            frequency = n_distinct(Invoice), monetary = sum(TotalSpend)) 

head(rfm_df)
```

```{r RFM Distribution, echo=TRUE, eval = TRUE}
rfm_df %>% 
  gather(type,value,recency:monetary) %>% 
  ggplot(aes(x = value, color = type, fill = type)) +
  geom_density(alpha = 0.6) +
  facet_wrap(~type, nrow = 1, scales="free") +
  labs(title = 'RFM Data Distribution')
```

# Method and Analysis
In this project, K-Means method was used to identify groups accross all customer. K-Means clustering is one type of unsupervised learning algorithms, which makes groups based on the distance between the points. 

## Data Transformation
As can be seen on RFM Data Distribution graph, the data is highly skewed especially on frequency and monetary. In order to get more sense on the data, the log transformation can be applied:
```{r Log RFM Distribution, echo=TRUE, eval = TRUE}
log_rfm <- rfm_df %>% 
  mutate(log_recency = log(recency), log_frequency = log(frequency), log_monetary = log(monetary))

log_rfm %>% 
  gather(type,value,log_recency:log_monetary) %>% 
  ggplot(aes(x = value, color = type, fill = type)) +
  geom_density(alpha = 0.6) +
  facet_wrap(~type, nrow = 1, scales="free") +
  labs(title = 'Log RFM Data Distribution')
```

Moreover, Due to the used of distance in K-Means method, the features unit scale is important. Hence it is required to do standardization and normalisation by finding z-score of features prior to do clustering. This can be done by calculating by using this formula :
$$  z = {x - \mu \over \sigma}$$
That calculation can be done by using scale function in R.
```{r Scale Log RFM Distribution, echo=TRUE, eval = TRUE, message = FALSE, warning = FALSE}
scale_df <- log_rfm %>% 
 mutate(scale_log_r = scale(log_recency), scale_log_f = scale(log_frequency), scale_log_m = scale(log_monetary))

scale_df %>% 
  gather(type,value,scale_log_r:scale_log_m) %>% 
  ggplot(aes(x = value, color = type, fill = type)) +
  geom_density(alpha = 0.6) +
  labs(title = 'Scaled Log RFM Data Distribution')
```

All the features already on the same scale after standardization and normalization. So, the preprocessing data has been done and clustering process can be performed by this scaled dataset.

## Clustering with K-Means

The first step in this clustering is to find the right number of cluster. This process can be done by make a elbow curve and choose the most optimized cluster based on that curve. Elbow curve is a curve that made by plotting Sum Square Error (SSE) from the K-Means alghoritm. This number represent the sum square value of the actual point distance to the central of each cluster. 
```{r Elbow Curve, echo=TRUE, eval = TRUE, message = FALSE, warning = FALSE}
# Iterate from 1 to 15 to find the most optimum cluster by elbow curve
set.seed(100)
used_var = c("scale_log_r","scale_log_f","scale_log_m")
sse <- sapply(1:15, 
              function(k)
              {
                kmeans(x=scale_df[used_var], k, nstart=25)$tot.withinss
              }
)

plot(sse, type = "o", xlab = "n - cluster", main = 'Elbow Curves')
```

Observing from the elbow curve, the most optimum cluster was pictured as the elbow of the curve somewhere SSE dramatically decrease but not to much. In this case 4 was choosen to be the most optimum cluster.

After decided the cluster numbers, a model can be build and make an actual cluster like below:
```{r Clustering 4, echo=TRUE, eval = TRUE}
# Calculate Cluster group with 4 cluster
segment_4 <- kmeans(x=scale_df[used_var], 4, nstart=25)
cluster <- as.factor(segment_4$cluster)
rfm_clustered4 <- cbind(scale_df,cluster)

rfm_clust4_summary <- rfm_clustered4 %>% 
  group_by(cluster) %>% 
  summarise(total = n_distinct(CustomerID), 
            average_recency = round(mean(recency),2),
            average_frequency = round(mean(frequency),2),
            average_monetary = round(mean(monetary),2)
  )

rfm_clust4_summary %>% knitr::kable()
```



```{r Clustering 4 Summary, echo=TRUE, eval = TRUE}
rfm_clust4_summary %>% 
  gather(key = 'measure', value = 'values',c(5,4,3,2)) %>% 
  ggplot(aes(x = cluster , y = values, fill = cluster)) +
  geom_col() +
  facet_wrap(~measure, ncol = 1, scales="free_y")
```

```{r Clustering 4 Snake, echo=TRUE, eval = TRUE}
rfm_clustered4 %>% 
  group_by(cluster) %>% 
  summarise(average_recency = round(mean(scale_log_r),2),
            average_frequency = round(mean(scale_log_f),2),
            average_monetary = round(mean(scale_log_m),2)) %>% 
  ggparcoord(columns = 2:4, groupColumn = 'cluster',
           showPoints = TRUE, 
           alphaLines = 0.3, title = "Cluster Snakeplot")
```

# Results
As a result, there are 4 categories of customers generated in this project. Cluster 1 can be categorize as most valuable customer. Customer in this category have spent most frequently and spent the most money. On the other Cluster 2 have less frequent and less value of money compared to Cluster 1. However they haven't transact recently. Cluster 3 recently have transaction but not too frequent and only spend small amount of money. This Cluster can be a new customer that just done the transaction. Lastly, cluster 4 become our loss customer with the least frequent and monetary value and havent done any transaction for a while.

As a summary the detail for each cluster can be seen as follow:

```{r Clustering 4 Result, echo=TRUE, eval = TRUE}
rfm_clust4_summary %>% knitr::kable()
```

\pagebreak

# Conclusion

Finnaly, a cluster have successfuly build and each Customer can be categories based on their recency, frequency and monetary values.  Furthermore this cluster also can be used as a basis to give different treatment to gain more benefit to the business. 

Further analysis also can be applied and introduce more variable like tenure or how many days since the customer doing first transaction to the last day of their transaction. More detail analysis also can be done by give more specific time range like RFM for yearl, monthly or weekly to see how our customer perform during that period.
